{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of tanh\n",
    "\n",
    "$$\\tanh x = { \\frac{\\sinh x}{\\cosh x} }\n",
    " = { \\frac {e^{x}-e^{-x}}{e^{x} +e^{-x}} } $$\n",
    "\n",
    "\n",
    "$$\\frac{d}{dx}{\\sinh x} = \\frac{1}{2}*(\\frac{d}{dx}e^{x}-\\frac{d}{dx}e^{-x}) = \\frac{1}{2}*(e^{x} + e^{-x}) = \\cosh x $$\n",
    "$$\\frac{d}{dx}{\\cosh x} = \\frac{1}{2}*(\\frac{d}{dx}e^{x}+\\frac{d}{dx}e^{-x}) = \\frac{1}{2}*(e^{x} - e^{-x}) = \\sinh x. $$\n",
    "<p style=\"text-align: center;\">  \n",
    "Applying the quotient rule:\n",
    "</p>\n",
    "\n",
    "$$ \\frac{d}{dx}{\\tanh x} = \\frac{\\cosh x \\frac{d}{dx} \\sinh x - \\sinh x \\frac{d}{dx} \\cosh x } {{\\cosh}^{2} x} $$\n",
    "\n",
    "$$ = \\frac{\\cosh x \\cosh x - \\sinh x \\sinh x}{\\cosh^{2} x}$$\n",
    "\n",
    "$$ = 1 - \\frac{\\sinh ^{2}x}{\\cosh^{2}x} = 1 - \\tanh^{2} x $$\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of logistic sigmoid\n",
    "\n",
    "$$ \\sigma (x) =  \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "<p style=\"text-align: center;\">  \n",
    "Applying the chain rule:\n",
    "</p>\n",
    "$$ \\frac{d}{dx} \\sigma (x) = - \\frac{1}{(1 + e^{-x})^{2}} * -e^{-x} $$\n",
    "\n",
    "$$ = \\sigma (x) \\frac{e^{-x}}{1 + e^{-x}} $$\n",
    "\n",
    "$$ = \\sigma (x) (1 - \\sigma (x)) $$\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ReLU\n",
    "\n",
    "The Rectified Linear Unit is defined as $ f(x)=\\text{max}(0,x) $.  \n",
    "We can rewrite this using two cases:\n",
    "\n",
    "\\begin{equation} \n",
    "f(x)=\n",
    "    \\begin{cases}\n",
    "      x, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation} .\n",
    "\n",
    "Upon simple differentiation of the two cases, we get\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation} \n",
    "\n",
    "The ReLU is discontinuous at x = 0, \n",
    "therefore its derivative at x = 0 is technically not defined;\n",
    "however, we are explicitly setting $ f'(0) = 0 $ in the statement above, so we have defined a derivative $\\forall x \\in \\mathbb{R}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backward Pass - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Derivative 1:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial W^{(2)}}$.\n",
    "\n",
    "By the chain rule:\n",
    "\n",
    "$ = \\frac{\\partial L}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\frac{\\partial z^{(2)}}{\\partial W^{(2)}} $\n",
    "\n",
    "### factor #1\n",
    "\n",
    "We are interested in the derivative of the Loss function with respect to the vector of activations $\\mathbf{a}^{(2)}$.  This is written as\n",
    "\n",
    "$\\nabla_{\\mathbf {a}^{(2)}} L(\\mathbf{y},\\mathbf{a}^{(2)})$.\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a^{(2)}}  =  -\\frac{1}{N}\\frac{\\partial}{\\partial a^{(2)}} \\sum\\limits_{n \\in N} \\sum\\limits_{i \\in C} y_{n,i} log {a}^{(2)}_{n,i}   $\n",
    "\n",
    "where $N$ is the number of data points and $C$ is the number of unique classes.  For our Make Moons dataset, $N = 200$ and $C = 2$\n",
    "\n",
    "To preserve all information, this derivative is an $ N \\times C $ matrix, where each element, $\\frac{\\partial L}{\\partial a^{(2)}_{n,i}}$, is \n",
    "\n",
    "$-\\frac{1}{N}\\frac{y_{n,i}}{a^{(2)}_{n,i}}$\n",
    "\n",
    "\n",
    "\n",
    "### factor #2\n",
    "We are interested in the derivative of the activations $\\mathbf{a}^{(2)}$ with respect to inputs $\\mathbf{z}^{(2)}$.  Above, we were computing the derivative of a scalar with respect to a matrix.  Now we are computing the derivative of a vector with respect to a vector, so we write the Jacobian matrix as \n",
    "\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}} = $$\n",
    "\n",
    "   \\begin{bmatrix}\n",
    "       \\frac{\\partial a^{(2)}_{0}}{\\partial z^{(2)}_{0}} &    \\frac{\\partial a^{(2)}_{0}}{\\partial z^{(2)}_{1}} \\\\\n",
    "       \\frac{\\partial a^{(2)}_{1}}{\\partial z^{(2)}_{0}} &    \\frac{\\partial a^{(2)}_{1}}{\\partial z^{(2)}_{1}}\n",
    "   \\end{bmatrix}\n",
    "\n",
    "\n",
    "For $ i,j \\in C$,\n",
    "$\\frac{\\partial a^{(2)}_{i}}{\\partial z^{(2)}_{j}}  = \\frac{\\partial}{\\partial z^{(2)}_{j}} softmax(\\mathbf{z}^{(2)})_{i}$\n",
    "\n",
    "$ = \\frac{\\partial}{\\partial z^{(2)}_{j}} \\frac{exp(z^{(2)}_{i})}{\\sum\\limits_{i \\in C}exp(z^{(2)}_{i})} $\n",
    "\n",
    "Now, we can use the quotient rule for each of the four elements in the matrix.  \n",
    "\n",
    "However, this is getting messy.  Let's try something different:  observe that \n",
    "\n",
    "### factors #1 and #2 together\n",
    "\n",
    "We can ignore N datapoints for now, and consider only the sum over classes $k \\in C$.\n",
    "\n",
    "Let's compute the following:\n",
    "$\\frac{\\partial L}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}} $\n",
    "\n",
    "Expressing the output layer's activations element-wise:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_{i}} = - \\sum\\limits_{k \\in C} y_{k} \\frac{\\partial log a_{k}}{\\partial z_{i}} = - \\sum\\limits_{k \\in C} y_{k} \\frac{1}{a_{k}} \\frac{\\partial  a_{k}}{\\partial z_{i}}$$\n",
    "\n",
    "\n",
    "Let's split the derivative into two cases:  \n",
    "\n",
    "#### case 1:  $ i = j$.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{i}}  = \\frac{\\partial}{\\partial z_{i}} \\frac{exp(z_{i})}{\\sum\\limits_{k \\in C}exp(z_{k})}$$\n",
    "\n",
    "By the quotient rule, we get:\n",
    "\n",
    "$$ \\frac{\\big[\\sum\\limits_{k}exp(z_{k})\\big]exp(z_{i}) - exp(z_{i})exp(z_{i}) }{ \\big[{\\sum\\limits_{k}exp(z_{k})}\\big]^{2} } $$.\n",
    "\n",
    "$$ =  \\frac{\\big[\\sum\\limits_{k}exp(z_{k})\\big] - exp(z_{i}) }{\\sum\\limits_{k}exp(z_{k})}  \\frac{exp(z_{i})}{\\sum\\limits_{k}exp(z_{k})} $$.\n",
    "\n",
    "This allows us to express the derivative of the softmax in terms of softmax function itself.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{i}} = [1 - softmax(z_{i})] softmax(z_{i})  = [1 - a_{i}]a_{i} $$\n",
    "\n",
    "Reincorporating into the full derivative above, we get:\n",
    "\n",
    "$$ - \\sum\\limits_{k = i} y_{k} \\frac{1}{a_{i}} \\frac{\\partial  a_{i}}{\\partial z_{i}}$$\n",
    "\n",
    "$$ =  -  y_{i} \\frac{1}{a_{i}} \\frac{\\partial  a_{i}}{\\partial z_{i}}$$\n",
    "\n",
    "$$ =  -  y_{i} \\frac{1}{a_{i}} [1 - a_{i}]a_{i}$$\n",
    "\n",
    "$$ = - y_{i} (1 - a_{i}) $$\n",
    "\n",
    "for i = j.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### case 2: $ i \\neq j$.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{j}}  = \\frac{\\partial}{\\partial z_{j}} \\frac{exp(z_{i})}{\\sum\\limits_{k \\in C}exp(z_{k})}$$\n",
    "\n",
    "Using the quotient rule again:\n",
    "\n",
    "$$ \\frac{0 * exp(z_{i}) - exp(z_{i})exp(z_{j}) }{ \\big[{\\sum\\limits_{k}exp(z_{k})}\\big]^{2} }  = -\\frac{exp(z_{i})}{\\sum\\limits_{k}exp(z_{k})} \\frac{exp(z_{j})}{\\sum\\limits_{k}exp(z_{k})} = - softmax(z_{i})softmax(z_{j}) = -a_{i}a_{j}$$.\n",
    "\n",
    "Incorporating this into the derivative above,\n",
    "\n",
    "$$ - \\sum\\limits_{i \\neq j \\in C} y_{j} \\frac{1}{a_{j}} \\frac{\\partial  a_{j}}{\\partial z_{i}} =  -\\sum\\limits_{i \\neq j \\in C} y_{j} \\frac{1}{a_{j}} (-a_{j} a_{i}) =  \\sum\\limits_{i \\neq j \\in C} y_{j} a_{i} $$\n",
    "\n",
    "#### combining two cases:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z_{i}} = - y_{i} (1 - a_{i}) + \\sum\\limits_{j \\neq i \\in C} y_{j} a_{i} $$\n",
    "$$ =  - y_{i} +  y_{i}a_{i} + \\sum\\limits_{j \\neq i \\in C} y_{j} a_{i} $$\n",
    "\n",
    "$$ = -y_{i} + \\sum\\limits_{j \\in C} y_{j}a_{i} $$\n",
    "\n",
    "Rearranging and then finally using the trick that y is a one-hot vector, we get \n",
    "$$ = a_{i}\\sum\\limits_{j \\in C}y_{j} - y_{i}  $$\n",
    "\n",
    "$$ = a_{i}* 1 - y_{i}   = a_{i} - y_{i}$$.\n",
    "\n",
    "So the **gradient** of the loss function with respect to a particular z_{i} is simply the difference between the true class identity in y and the activation value a_{i} at that output neuron.\n",
    "\n",
    "\n",
    "Composing functions, so as to compute the cross-entropy of the softmax before taking the derivative, we get:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}_{i}} = -\\frac{1}{N}\\frac{\\partial}{\\partial \\mathbf{z}^{(2)}_{i}} \\sum\\limits_{n \\in N} \\sum\\limits_{i \\in C} y_{i,n}log (softmax (\\mathbf{z}^{(2)}_{i,n})) = -\\frac{1}{N}\\frac{\\partial}{\\partial \\mathbf{z}^{(2)}_{i}} \\big[ y_{0,0} log(softmax(z_{0,0}^{(2)})) + y_{1,0} log(softmax(z_{1,0}^{(2)})) +\n",
    "y_{0,1} log(softmax(z_{0,1}^{(2)})) + y_{1,1} log(softmax(z_{1,1}^{(2)})) + \\dots + y_{C,N} log(softmax(z_{C,N}^{(2)})) \\big]$\n",
    "\n",
    "Using the definition of the softmax, we get nice cancelation of logs and exponents.\n",
    "\n",
    "$ = -\\frac{1}{N}\\frac{\\partial}{\\partial \\mathbf{z}^{(2)}_{i}} \\bigg[ y_{0,0} [z_{0,0}^{(2)} -  log \\sum\\limits_{j}exp(z_{j,0}^{(2)})] + y_{1,0} [z_{1,0}^{(2)} -  log \\sum\\limits_{j}exp(z_{j,0}^{(2)})] +\n",
    "y_{0,1} [z_{0,1}^{(2)} -  log \\sum\\limits_{j}exp(z_{j,0}^{(2)})] + y_{1,1} [z_{1,1}^{(2)} -  log \\sum\\limits_{j}exp(z_{j,0}^{(2)})] + \\dots + y_{C,N} [z_{C,N}^{(2)} -  log \\sum\\limits_{j}exp(z_{j,0}^{(2)})]  \\bigg]$\n",
    "\n",
    "Factoring out the log sum terms, and realizing that we have a dot product left over, we get:\n",
    "$ =  \\frac{1}{N}\\frac{\\partial}{\\partial \\mathbf{z}^{(2)}_{i}} [ log \\sum\\limits_{j}exp(z_{j,0}^{(2)}) \\mathbf{y} - \\mathbf{z} \\cdot \\mathbf{y} ] $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### factor #3\n",
    "\n",
    "### putting it together:\n",
    "\n",
    "$N \\times C$, $C \\times $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative 2:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial b_{2}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative 3:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial W_{1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative 4:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial b_{1}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
