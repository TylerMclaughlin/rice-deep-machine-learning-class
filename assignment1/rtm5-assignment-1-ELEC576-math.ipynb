{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 for ELEC576 - Deep Machine Learning\n",
    "## Author:  R. Tyler McLaughlin\n",
    "## Department: SSPB\n",
    "## Date:  10/02/17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](make_moons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of tanh\n",
    "\n",
    "$$\\tanh x = { \\frac{\\sinh x}{\\cosh x} }\n",
    " = { \\frac {e^{x}-e^{-x}}{e^{x} +e^{-x}} } $$\n",
    "\n",
    "\n",
    "$$\\frac{d}{dx}{\\sinh x} = \\frac{1}{2}*(\\frac{d}{dx}e^{x}-\\frac{d}{dx}e^{-x}) = \\frac{1}{2}*(e^{x} + e^{-x}) = \\cosh x $$\n",
    "$$\\frac{d}{dx}{\\cosh x} = \\frac{1}{2}*(\\frac{d}{dx}e^{x}+\\frac{d}{dx}e^{-x}) = \\frac{1}{2}*(e^{x} - e^{-x}) = \\sinh x. $$\n",
    "<p style=\"text-align: center;\">  \n",
    "Applying the quotient rule:\n",
    "</p>\n",
    "\n",
    "$$ \\frac{d}{dx}{\\tanh x} = \\frac{\\cosh x \\frac{d}{dx} \\sinh x - \\sinh x \\frac{d}{dx} \\cosh x } {{\\cosh}^{2} x} $$\n",
    "\n",
    "$$ = \\frac{\\cosh x \\cosh x - \\sinh x \\sinh x}{\\cosh^{2} x}$$\n",
    "\n",
    "$$ = 1 - \\frac{\\sinh ^{2}x}{\\cosh^{2}x} = 1 - \\tanh^{2} x $$\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of logistic sigmoid\n",
    "\n",
    "$$ \\sigma (x) =  \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "<p style=\"text-align: center;\">  \n",
    "Applying the chain rule:\n",
    "</p>\n",
    "$$ \\frac{d}{dx} \\sigma (x) = - \\frac{1}{(1 + e^{-x})^{2}} * -e^{-x} $$\n",
    "\n",
    "$$ = \\sigma (x) \\frac{e^{-x}}{1 + e^{-x}} $$\n",
    "\n",
    "$$ = \\sigma (x) (1 - \\sigma (x)) $$\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ReLU\n",
    "\n",
    "The Rectified Linear Unit is defined as $ f(x)=\\text{max}(0,x) $.  \n",
    "We can rewrite this using two cases:\n",
    "\n",
    "\\begin{equation} \n",
    "f(x)=\n",
    "    \\begin{cases}\n",
    "      x, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation} .\n",
    "\n",
    "Upon simple differentiation of the two cases, we get\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation} \n",
    "\n",
    "The ReLU is discontinuous at x = 0, \n",
    "therefore its derivative at x = 0 is technically not defined;\n",
    "however, we are explicitly setting $ f'(0) = 0 $ in the statement above, so we have defined a derivative $\\forall x \\in \\mathbb{R}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backward Pass - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Derivative 1:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial W^{(2)}}$.\n",
    "\n",
    "By the chain rule:\n",
    "\n",
    "$ = \\frac{\\partial L}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\frac{\\partial z^{(2)}}{\\partial W^{(2)}} $\n",
    "\n",
    "However, there is a mathematical trick where we can speed up backpropagation computations if we **compose** the softmax and cross-entropy.  This means we'll be looking at factors #1 and #2 together, $\\frac{\\partial L}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = \\frac{\\partial L}{\\partial z^{(2)}}$.  We will later add factor #3 $\\frac{\\partial z^{(2)}}{\\partial W^{(2)}}$ \n",
    "\n",
    "This trick was taken from the \"Deep Learning\" textbook by Goodfellow, Bengio, and Courville, page 199.\n",
    "\n",
    "### factors #1 and #2 together\n",
    "\n",
    "We can ignore N datapoints for now, and consider only the sum over classes $k \\in C$.\n",
    "\n",
    "Let's compute the following:\n",
    "$\\frac{\\partial L}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}} $\n",
    "\n",
    "Expressing the output layer's activations element-wise:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_{i}} = - \\sum\\limits_{k \\in C} y_{k} \\frac{\\partial log a_{k}}{\\partial z_{i}} = - \\sum\\limits_{k \\in C} y_{k} \\frac{1}{a_{k}} \\frac{\\partial  a_{k}}{\\partial z_{i}}$$\n",
    "\n",
    "\n",
    "Let's split the derivative into two cases:  \n",
    "\n",
    "#### case 1:  $ i = j$.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{i}}  = \\frac{\\partial}{\\partial z_{i}} \\frac{exp(z_{i})}{\\sum\\limits_{k \\in C}exp(z_{k})}$$\n",
    "\n",
    "By the quotient rule, we get:\n",
    "\n",
    "$$ \\frac{\\big[\\sum\\limits_{k}exp(z_{k})\\big]exp(z_{i}) - exp(z_{i})exp(z_{i}) }{ \\big[{\\sum\\limits_{k}exp(z_{k})}\\big]^{2} } $$.\n",
    "\n",
    "$$ =  \\frac{\\big[\\sum\\limits_{k}exp(z_{k})\\big] - exp(z_{i}) }{\\sum\\limits_{k}exp(z_{k})}  \\frac{exp(z_{i})}{\\sum\\limits_{k}exp(z_{k})} $$.\n",
    "\n",
    "This allows us to express the derivative of the softmax in terms of softmax function itself.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{i}} = [1 - softmax(z_{i})] softmax(z_{i})  = [1 - a_{i}]a_{i} $$\n",
    "\n",
    "Reincorporating into the full derivative above, we get:\n",
    "\n",
    "$$ - \\sum\\limits_{k = i} y_{k} \\frac{1}{a_{i}} \\frac{\\partial  a_{i}}{\\partial z_{i}}$$\n",
    "\n",
    "$$ =  -  y_{i} \\frac{1}{a_{i}} \\frac{\\partial  a_{i}}{\\partial z_{i}}$$\n",
    "\n",
    "$$ =  -  y_{i} \\frac{1}{a_{i}} [1 - a_{i}]a_{i}$$\n",
    "\n",
    "$$ = - y_{i} (1 - a_{i}) $$\n",
    "\n",
    "for i = j.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### case 2: $ i \\neq j$.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{j}}  = \\frac{\\partial}{\\partial z_{j}} \\frac{exp(z_{i})}{\\sum\\limits_{k \\in C}exp(z_{k})}$$\n",
    "\n",
    "Using the quotient rule again:\n",
    "\n",
    "$$ \\frac{0 \\cdot exp(z_{i}) - exp(z_{i})exp(z_{j}) }{ \\big[{\\sum\\limits_{k}exp(z_{k})}\\big]^{2} }  = -\\frac{exp(z_{i})}{\\sum\\limits_{k}exp(z_{k})} \\frac{exp(z_{j})}{\\sum\\limits_{k}exp(z_{k})} = - softmax(z_{i})softmax(z_{j}) = -a_{i}a_{j}$$.\n",
    "\n",
    "Incorporating this into the derivative above,\n",
    "\n",
    "$$ - \\sum\\limits_{i \\neq j \\in C} y_{j} \\frac{1}{a_{j}} \\frac{\\partial  a_{j}}{\\partial z_{i}} =  -\\sum\\limits_{i \\neq j \\in C} y_{j} \\frac{1}{a_{j}} (-a_{j} a_{i}) =  \\sum\\limits_{i \\neq j \\in C} y_{j} a_{i} $$.\n",
    "\n",
    "#### combining two cases:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z_{i}} = - y_{i} (1 - a_{i}) + \\sum\\limits_{j \\neq i \\in C} y_{j} a_{i} $$\n",
    "$$ =  - y_{i} +  y_{i}a_{i} + \\sum\\limits_{j \\neq i \\in C} y_{j} a_{i} $$\n",
    "\n",
    "$$ = -y_{i} + \\sum\\limits_{j \\in C} y_{j}a_{i} $$\n",
    "\n",
    "Rearranging and then finally realizing that since y is a one-hot vector, it sums to 1, we get \n",
    "$$ = a_{i}\\sum\\limits_{j \\in C}y_{j} - y_{i}  $$\n",
    "\n",
    "$$ = a_{i} \\cdot 1 - y_{i}   = a_{i}^{(2)} - y_{i}$$.\n",
    "\n",
    "So the **gradient** of the loss function with respect to a particular z_{i} is quite simple mathematically and conceptually.  It is the difference between the true class identity pre-specified in y and the current activation a_{i} found by training at the ith ouput neuron. \n",
    "\n",
    "### factor #3\n",
    "\n",
    "We have $\\frac{\\partial L}{\\partial z_{i}^{(2)}}$ and we want the matrix $ \\frac{\\partial L}{\\partial W^{(2)}} $.  The chain rule tells us that $ \\frac{\\partial L}{\\partial W_{w}^{(2)}} = \\sum\\limits_{i}\\frac{\\partial L}{\\partial z_{i}^{(2)}}\\frac{\\partial z_{i}^{(2)}}{\\partial W_{w}^{(2)}},$ where $w$ represents a tuple of two indices (flattened matrix/tensor notation). \n",
    "\n",
    "Thus, to finish this part, we need $\\frac{\\partial z_{i}^{(2)}}{\\partial W_{w}}$.\n",
    "\n",
    "The connection between the hidden layer to the input of the output layer is given in terms of the activation at each of the hidden neurons, plus the weights, and biases:\n",
    "\n",
    "$ \\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.$\n",
    "\n",
    "$ \\mathbf{a}^{(2)} $  is an $N \\times H$ matrix and $\\mathbf{W}^{(2)}$  is an $H \\times C$ matrix, where N is the number of data points, H is the number of hidden layers, and C is the number of categories (output neurons).\n",
    "\n",
    "$ \\mathbf{z}^{(2)} = \\begin{bmatrix} a_{0,0} & a_{0,1} & \\dots & a_{0,H} \\\\ \\vdots  & \\ddots & & \\vdots \\\\ a_{N,0} & \\dots & & a_{N,H} \\end{bmatrix} \\begin{bmatrix} W_{0,0} & W_{0,1} & \\dots & W_{0,C} \\\\ W_{1,0} & W_{1,1} & & W_{1,C} \\\\  \\vdots & & \\ddots & \\vdots \\\\ W_{H,0}  & \\dots  & & W_{H,C}   \\end{bmatrix}  + \\mathbf{b}^{(2)} $ \n",
    "\n",
    "$ = \\begin{bmatrix} a_{0,0}W_{0,0} + a_{0,1}W_{1,0} + a_{0,2}W_{2,0} + ... + a_{0,H}W_{H,0} & \\dots & a_{0,0}W_{0,C} + a_{0,1}W_{1,C} + a_{0,2}W_{2,C} + ... + a_{0,H}W_{H,C} \\\\ \\vdots & \\ddots & \\vdots \\\\ \n",
    "a_{N,0}W_{0,0} + a_{N,1}W_{1,0} + a_{N,2}W_{2,0} + ... + a_{N,H}W_{H,0} & \\dots & a_{N,0}W_{0,C} + a_{N,1}W_{1,C} + a_{N,2}W_{2,C} + ... + a_{N,H}W_{H,C}\\end{bmatrix} + \\mathbf{b}^{(2)} $\n",
    "\n",
    "We only care about $z_{i}^{(2)}$, not the full matrix $\\mathbf{z}^{(2)}$ so we can consider the ith column of this matrix:\n",
    "\n",
    "$\\begin{bmatrix} a_{0,0}W_{0,i} + a_{0,1}W_{1,i} + a_{0,2}W_{2,i} + ... + a_{0,H}W_{H,i}  \\\\ \\vdots \\\\ a_{N,0}W_{0,i} + a_{N,1}W_{1,i} + a_{N,2}W_{2,i} + ... + a_{N,H}W_{H,i}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "We can calculate $\\frac{\\partial z_{i}^{(2)}}{\\partial W_{w}}$ by considering two cases, depending on the tuple of indices $w$.\n",
    "\n",
    "#### case 1:  $w = ( h, j), h \\in H, j = i $\n",
    "\n",
    "Taking the derivative causes every term  to vanish in each element of the column vector $z_{i}^{(2)}$ *except for a single term*.  This term is equal to a_{n,h}.\n",
    "\n",
    "\n",
    "\n",
    "#### case 2:  $w = ( h, j), h \\in H, j \\neq i$\n",
    "\n",
    "In this case, the derivative is simply equal to zero.\n",
    "\n",
    "#### full matrix of partial derivatives of $z_{j}$ with respect to the weights\n",
    "\\begin{equation} \n",
    "\\frac{\\partial z_{i}^{(2)}}{\\partial W_{h,j}}=\n",
    "    \\begin{cases}\n",
    "      a_{h}^{(1)}, & \\text{if}\\ j = i \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation} .\n",
    "\n",
    "\n",
    "### Finishing the chain rule for $\\frac {\\partial L }{\\partial \\mathbf{W}^{(2)}} $\n",
    "\n",
    "Recall, we are trying to find the derivative of the loss function with respect to each of the weights.\n",
    "\n",
    "$$\\frac{\\partial L}{W_{i,j}} = \\sum\\limits_{k}\\frac{\\partial L}{\\partial z^{(2)}_{k}}\\frac{\\partial z^{(2)}_{k}}{\\partial W_{i,j}} $$\n",
    "\n",
    "We may write this compactly as $ \\nabla_\\mathbf{W} L.$\n",
    "\n",
    "Let's run through a few example entries in this matrix to get a feel for the pattern.\n",
    "\n",
    "#### example entry:  $W_{0,0}$\n",
    "\n",
    "$$\\frac{\\partial L}{W_{0,0}} = \\sum\\limits_{k}\\frac{\\partial L}{\\partial z^{(2)}_{k}}\\frac{\\partial z^{(2)}_{k}}{\\partial W_{0,0}}$$ \n",
    "\n",
    "Let's expand the sum over k.  Our multi-layer neural network has only two possible values for k (two classes), so the sum is simple:\n",
    "\n",
    "$$ = \\frac{\\partial L}{\\partial z^{(2)}_{0}}\\frac{\\partial z^{(2)}_{0}}{\\partial W_{0,0}} + \\frac{\\partial L}{\\partial z^{(2)}_{1}}\\frac{\\partial z^{(2)}_{1}}{\\partial W_{0,0}}$$ \n",
    "\n",
    "Let's next substitute what we derived above analytically for the first partial derivative:\n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0})\\frac{\\partial z^{(2)}_{0}}{\\partial W_{0,0}}  + (a_{1}^{(2)} - y_{1})\\frac{\\partial z^{(2)}_{1}}{\\partial W_{0,0}}$$  \n",
    "\n",
    "and then substitute what we derived for the second partial derivative:\n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0}) \\cdot a_{0}^{(1)}  + (a_{1}^{(2)} - y_{1}) \\cdot 0$$  \n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0}) \\cdot a_{0}^{(1)}  .$$\n",
    "\n",
    "OK great.  Let's take a look at another element of $ \\nabla_\\mathbf{W} L.$\n",
    "\n",
    "#### example entry:  $W_{0,1}$\n",
    "\n",
    "$$ \\frac{\\partial L}{W_{0,1}} = \\frac{\\partial L}{\\partial z^{(2)}_{0}}\\frac{\\partial z^{(2)}_{0}}{\\partial W_{0,1}} + \\frac{\\partial L}{\\partial z^{(2)}_{1}}\\frac{\\partial z^{(2)}_{1}}{\\partial W_{0,1}}$$ \n",
    "\n",
    "Substitution is straightforward.  Observe how the cancellation is different:\n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0})\\cdot 0  + (a_{1}^{(2)} - y_{1}) \\cdot a_{0}^{(1)} $$  \n",
    "\n",
    "$$ = (a_{1}^{(2)} - y_{1})\\cdot a_{0}^{(1)} $$  \n",
    "\n",
    "We can do one more example before generalizing and constructing the full matrix:\n",
    "\n",
    "#### example entry:  $W_{1,0}$\n",
    "\n",
    "$$ \\frac{\\partial L}{W_{1,0}} = \\frac{\\partial L}{\\partial z^{(2)}_{0}}\\frac{\\partial z^{(2)}_{0}}{\\partial W_{1,0}} + \\frac{\\partial L}{\\partial z^{(2)}_{1}}\\frac{\\partial z^{(2)}_{1}}{\\partial W_{1,0}}$$ \n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0})\\frac{\\partial z^{(2)}_{0}}{\\partial W_{1,0}}  + (a_{1}^{(2)} - y_{1})\\frac{\\partial z^{(2)}_{1}}{\\partial W_{1,0}}$$  \n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0})\\cdot a_{1}  + (a_{1}^{(2)} - y_{1})\\cdot 0 $$  \n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0}) \\cdot a_{1}^{(1)} $$\n",
    "\n",
    "#### generalizing:\n",
    "\n",
    "$$ (\\nabla_\\mathbf{W} L)_{i,j}  =  \\frac{\\partial L}{\\partial W_{i,j}^{(2)} } = (a_{j}^{(2)} - y_{j}) \\cdot a_{i}^{(1)}$$,\n",
    "\n",
    "for $ i \\in C,$ the number of output layers, and $j \\in H ,$ the number of hidden layers.\n",
    "\n",
    "This can be succinctly written in matrix form!\n",
    "\n",
    "$$ (\\nabla_\\mathbf{W} L)  =  \\frac{\\partial L}{\\partial \\mathbf{W^{(2)}} } = \\mathbf{a^{(1)}}^\\top \\cdot (\\mathbf{a^{(2)}} - \\mathbf{y}) $$,\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative 2:\n",
    "\n",
    "Calculate $ \\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}},$  where $\\mathbf{b}^{(2)}$ is a $ 1 \\times C$ bias matrix.\n",
    "\n",
    "### chain rule\n",
    "\n",
    "$$\\frac{\\partial L}{b_{i}^{(2)}} = \\sum\\limits_{k \\in C}\\frac{\\partial L}{\\partial z^{(2)}_{k}}\\frac{\\partial z^{(2)}_{k}}{\\partial b_{i}^{(2)}} = (a_{0}^{(2)} - y_{0})\\frac{\\partial z^{(2)}_{0}}{\\partial b_{i}^{(2)}}  + (a_{1}^{(2)} - y_{1})\\frac{\\partial z^{(2)}_{1}}{\\partial b_{i}^{(2)}}$$\n",
    "\n",
    "Recall that $\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\cdot \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} $.\n",
    "\n",
    "If we let $ \\mathbf{q} = \\mathbf{a}^{(1)} \\cdot \\mathbf{W}^{(2)} $, then  $\\mathbf{z}^{(2)} = \\begin{bmatrix} q_{0,0} & \\dots & q_{0,C} \\\\\n",
    "\\vdots  & \\ddots &  \\vdots \\\\\n",
    "q_{N,0} & \\dots & q_{N,C}\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_{0}^{(2)} & \\dots & b_{C}^{(2)}\\end{bmatrix} $ \n",
    "\n",
    "Note:  technically $\\mathbf{b}^{(2)}$ should be an $ N \\times C$ dimensional matrix to get the summed terms to agree, but NumPy doesn't care about this and repeats (a.k.a. recycles) along the $N$ rows, yielding:\n",
    "\n",
    "$\\mathbf{z}^{(2)} = \\begin{bmatrix} q_{0,0} + b_{0}^{(2)} & \\dots & q_{0,C} + b_{C}^{(2)} \\\\\n",
    "\\vdots  & \\ddots &  \\vdots \\\\\n",
    "q_{N,0} + b_{0}^{(2)} & \\dots & q_{N,C} + b_{C}^{(2)}\n",
    "\\end{bmatrix} .$  \n",
    "\n",
    "So each $ z_{i}^{(2)} $ is a column of this matrix.   We note that $z_{i}^{(2)}$ does not depend on $b_{j}$ for $i \\neq j$ and thus $ \\frac{\\partial z^{(2)}_{i}}{\\partial b_{j}^{(2)}} = 0.$  Otherwise, if $ i = j$, then $\\frac{\\partial z^{(2)}_{i}}{\\partial b_{i}^{(2)}} = 1.$ \n",
    "\n",
    "### substituting into the chain rule:\n",
    "\n",
    "Just as we did before for $\\frac{\\partial L}{\\partial W_{i,j}}$, let's look at a few specific cases before generalizing.\n",
    "\n",
    "$$\\frac{\\partial L}{b_{i}^{(2)}} = \\sum\\limits_{k \\in C}\\frac{\\partial L}{\\partial z^{(2)}_{k}}\\frac{\\partial z^{(2)}_{k}}{\\partial b_{i}^{(2)}} = (a_{0}^{(2)} - y_{0})\\frac{\\partial z^{(2)}_{0}}{\\partial b_{i}^{(2)}}  + (a_{1}^{(2)} - y_{1})\\frac{\\partial z^{(2)}_{1}}{\\partial b_{i}^{(2)}} + \\dots + (a_{C}^{(2)} - y_{C})\\frac{\\partial z^{(2)}_{1}}{\\partial b_{i}^{(2)}}$$.  Every term vanishes except where $ i = j$, in which case, that term is equal to $(a_{i}^{(2)} - y_{i})$.  \n",
    "\n",
    "Thus, $\\frac{\\partial L}{b_{i}^{(2)}} = (a_{i}^{(2)} - y_{i})$.\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative 3:\n",
    "\n",
    "Here we calculate $ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}}$, where $\\mathbf{W}^{(1)}$ is an $I \\times H$ matrix with $H$ representing the number of hidden nodes and $I$ representing the number of input nodes.  This derivative is also a matrix.\n",
    "\n",
    "Let $a$ be the element-wise activation function.  Let $a'$ be its derivative.\n",
    "\n",
    "First, we can work on the gradient of $ z_{i}^{(2)} $ (the input to arbitrary node in layer 2) with respect to  $ z_{j}^{(1)}$ (the input to arbitrary node in layer 1).\n",
    "\n",
    "$$ \\frac{\\partial z_{i}^{(2)}}{\\partial z_{j}^{(1)}} = \\sum\\limits_{p}\\frac{\\partial z_{i}^{(2)}}{\\partial a_{p} } \\frac{\\partial a_{p}}{\\partial z_{j}^{(1)}}  $$\n",
    "\n",
    "where $p$ is a tuple of indices and $a$ is the $N x H$ matrix of activations.\n",
    "\n",
    "Recall the equation for the input to the second layer: \n",
    "$$\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} $$\n",
    "\n",
    "If we take a  single index of z, we get the following:\n",
    "\n",
    "$$ z_{i}^{(2)} = z  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our Three Layer Neural Network\n",
    "\n",
    "## Experimenting with Activation Functions\n",
    "\n",
    "### Hyperbolic Tangent Activation Function\n",
    "![](1e_figures/Figure_1.E.1.tanh.png)\n",
    "\n",
    "### Sigmoid Activation Function\n",
    "\n",
    "![](1e_figures/Figure_1.E.1.sigmoid.png)\n",
    "\n",
    "This, to my eyes, looks **identical** to using the tangent activation!\n",
    "\n",
    " Note:  I tried re-implementing the sigmoid function calculation with a stable algorithm (Scipy built-in) to avoid overflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Poincare/kinsen/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "z = -1e3\n",
    "# OLD CALCULATION\n",
    "activation = 1. / (1 + np.exp(-z))  # yields overflow for z = -1e3\n",
    "activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEW CALCULATION using Scipy.special.expit\n",
    "activation = scipy.special.expit(z)  # does not overflow\n",
    "activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Function\n",
    "\n",
    "![](1e_figures/Figure_1.E.1.relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Varying the number of hidden nodes\n",
    "\n",
    "### 1 Node\n",
    "![](1e_figures/Figure_1.E.1.tanh.1-nodes.png)\n",
    "\n",
    "1 node does as well as a simple multiple regression.\n",
    "### 2 Nodes\n",
    "![](1e_figures/Figure_1.E.1.tanh.2-nodes.png)\n",
    "\n",
    "2 nodes are able to fit the data slightly better than a single linear decision split.\n",
    "### 4 Nodes\n",
    "![](1e_figures/Figure_1.E.1.tanh.4-nodes.png)\n",
    "\n",
    "At 4 nodes, we see a proper fit to the crescent shape of the data.\n",
    "\n",
    "### 6 Nodes\n",
    "![](1e_figures/Figure_1.E.1.tanh.6-nodes.png)\n",
    "\n",
    "6 nodes gives a lower loss than 4 nodes, classifying the points more accurately, although at this point we appear to be overfitting to the random jitter.\n",
    "\n",
    "### 9 Nodes\n",
    "![](1e_figures/Figure_1.E.1.tanh.9-nodes.png)\n",
    "\n",
    "Adding more than 6 hidden nodes does not appear to improve the quality of the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 f:  Building a Deep Neural Network\n",
    "\n",
    "\n",
    "## Implementation Notes\n",
    "The code \"n_layer_neural_network-final.py\" accepts input that specifies the number of nodes per layer in the form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [X.shape[1], 3, 3, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example prepares for the instantiation of a multilayer perceptron with 4 hidden layers with 3 nodes each.\n",
    "The next example builds a network with 10 nodes in the first hidden layer, 9 nodes in the second, and so on, decreasing until 2 nodes are reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [X.shape[1], 10, 9, 8, 7, 6, 5, 4, 3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Keeping a dictionary of activation functions outside of the classes really helped with compartmentalization of the code.  So too did using separate classes for the general MLP layer and the final, Output layer.  Initially, I had tried to use if else control statements to do different things if the flag `last_layer` was `True`.\n",
    "\n",
    "I had to implement feedforward AND backprop for every layer AND also in `class DeepNeuralNetwork`.  I inherited from `class NeuralNetwork`, but by the end of implementing everything, I only really inherited the visualize_decision_boundary function.  \n",
    "\n",
    "`class OutputLayer` inherits from `class Layer` and if I recall correctly, only  methods actFun, diff_ActFun, and backprop were overridden. \n",
    "\n",
    "\n",
    "I used I used Layer.feedforward to implement DeepNeuralNetwork.feedforward.\n",
    "\n",
    "Analogously, I used Layer.backprop to implement DeepNeuralNetwork.backprop.  \n",
    "The `reversed` function came in handy for iterating over layers in reverse order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def backprop(self, y):\n",
    "        delta_term = y\n",
    "        for layer in reversed(self.layers):\n",
    "            delta_term = layer.backprop(delta_term)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I decided not to add $L^{2}$-norm regularization terms to the final loss.  I used regularization during backprop.  To be more consistent and more resistant to overfitting, an addition regularization step would have been helpful in theory, but my fits looked pretty good already without them.  Plus I believe I'd have to iterate over all Weights from all layers, so this would slow down my computation time a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the MLP creation involves looping over the layer_sizes, creating many regular, fully-connected layers, then appending a hidden layer class which has the softmax activation function.\n",
    "\n",
    "## Performance with respect to depth\n",
    "\n",
    "Let's try a deeper network, with 3 hidden nodes in two hidden layers (6 hidden nodes total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [X.shape[1], 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1f_figures/mlp_depth-3_3.png)\n",
    "This just looks a bit like a **smoother version** of the shallow network.  What if we increase the number of nodes while keeping the layers fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [X.shape[1], 6, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1f_figures/mlp_depth-6_6.png)\n",
    "\n",
    "This network with two hidden layers, and 6 nodes per layer is brilliant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [X.shape[1], 10, 8, 6, 4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1f_figures/mlp_depth-10-8-6-4-3.png)\n",
    "\n",
    "And this one is quite bad....  The extra depth doesn't seem to help with 20000 iterations.  What if instead of shrinking deeper layers, we fan out, increasing the size of the hidden layers near the output layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [X.shape[1], 3, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1f_figures/mlp_depth-3-4-6-8-10.png)\n",
    "\n",
    "Well this is much better, but I wonder how it fares with more than 20,000 iterations.  I'll try increasing to 80,000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1f_figures/mlp_depth-3-4-6-8-10_80k_runtime.png)\n",
    "\n",
    "Ok, cool we can see a bit of underlying the crescent shape!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### varying the nonlinearity\n",
    "\n",
    "![](1f_figures/mlp_depth-6_6_sigmoid.png)\n",
    "\n",
    "This looks **basically identical** to the tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Data Sets\n",
    "\n",
    "First I tried fitting to the Load_Wines data set from Scikit-learn.\n",
    "![](1f_figures/plot_wine_data.png)\n",
    "\n",
    "This data set is plottable.  Unfortunately, I was unable to visualize the decision boundary because there are too many features (about 13!).\n",
    "\n",
    "The same goes for the breast cancer dataset, which has 30 features.   For reference, the Make Moons dataset has two features.\n",
    "\n",
    "What I wanted was a dataset with more than two classes, however, because I was curious about how the deep neural net would perform with distinguishing three or four classes.\n",
    "\n",
    "Let's use the **Make Blobs** dataset because it give flexibility over the number of \"centers\" of the blobs.\n",
    "\n",
    "For all the following images, I ran with the same hyperparameters:  80,000 iterations and a learning rate $\\epsilon$ of 0.01.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = generate_data_blobs(centers=3)\n",
    "layer_sizes = [X.shape[1], 16, 16, 16, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1f_figures/blobs_3_fiveLayers)\n",
    "\n",
    "This looks like it's not converging, OR maybe it's overfitting.   Either way, let's try reducing the number of layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = generate_data_blobs(centers=3)\n",
    "layer_sizes = [X.shape[1], 16, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](1f_figures/blobs_3_threeLayers)\n",
    "\n",
    "Better fit with the same number of iterations.  Let's see what happens when we increase the number of blob centers aka classes to 4 classes.\n",
    "\n",
    "![](1f_figures/blobs_4_threeLayers)\n",
    "\n",
    "Increasing to 6 classes:\n",
    "\n",
    "![](1f_figures/blobs_6_threeLayers)\n",
    "\n",
    "And lastly, 12 classes:\n",
    "\n",
    "![](1f_figures/blobs_12_threeLayers)\n",
    "\n",
    "\n",
    "This neural network seems to have strong capacity to fit a large number of classes.  The only \"problem area\" is in the center of the plot, where the points become congested.\n",
    "\n",
    "To look at one final neural architecture, let's try changing the activation function to Sigmoid while maintainining the same depth and number of nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_sizes = [X.shape[1], 16,16]\n",
    "actFun_type ='sigmoid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1f_figures/blobs_12_threeLayers_sigmoid)\n",
    "\n",
    "Wow.  Sigmoid actually fits the data better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Training a Simple Deep Convolutional Network on MNIST\n",
    "\n",
    "## Part a) Build and Train a 4-layer DCN\n",
    "\n",
    "### conv1(5-5-1-32) - ReLU - maxpool(2-2) - conv2(5-5-32-64) - ReLU - maxpool(2-2) fc(1024) - ReLU - DropOut(0.5) - Softmax(10)\n",
    "\n",
    "All my code for this section is in the file dcn_mnist_part2a.py\n",
    "\n",
    "Last few lines of the terminal output:\n",
    "\n",
    "step 4900, training accuracy 0.98\n",
    "step 5000, training accuracy 0.96\n",
    "step 5100, training accuracy 1\n",
    "step 5200, training accuracy 1\n",
    "step 5300, training accuracy 0.96\n",
    "step 5400, training accuracy 1\n",
    "test accuracy 0.9869\n",
    "The training takes 1014.218075 second to finish\n",
    "\n",
    "About 17 minutes to run!  Not bad on my macbook pro.\n",
    "\n",
    "After running the file I moved my results directory to results_part_2a/\n",
    "\n",
    "#### visualized results\n",
    "This shows the training loss value as a function of iteration number.\n",
    "![](2a_figures/scalars.png)\n",
    "\n",
    "#### Computational Graph\n",
    "![](2a_figures/graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b) More on Visualizing Your Training\n",
    "\n",
    "### Statistics\n",
    "Because we don't want this report to get too gigantic, let's show some **select figures**.  If you're interested, all the figures are on my Github page https://github.com/TylerMclaughlin/rice-deep-machine-learning-class/tree/master/assignment1/2b_figures/\n",
    "\n",
    "#### Statistics for the weights for convolutional layer 1\n",
    "![](2b_figures/W_conv1.png)\n",
    "#### Statistics for the biases for convolutional layer 1\n",
    "![](2b_figures/b_conv1.png)\n",
    "\n",
    "#### Statistics for the net inputs to convolutional layer 1\n",
    "![](2b_figures/input_1.png)\n",
    "\n",
    "#### Statistics for the ReLU activations of convolutional layer 1\n",
    "![](2b_figures/h_conv1.png)\n",
    "\n",
    "#### Statistics for the max-pooled activations of convolutional layer 1\n",
    "![](2b_figures/h_pool1.png)\n",
    "\n",
    "#### Example histogram for the weights for convolutional layer 1\n",
    "![](2b_figures/W_conv1_histogram.png)\n",
    "\n",
    "\n",
    "That the min of some parameters is at zero the entire session means that the neuron or synapse is dead.  :|\n",
    "\n",
    "### Validation and Test Set Accuracy\n",
    "\n",
    "Summaries for test and validation set prediction accuracy were output every epoch, so thus there are only 5 data points:\n",
    "#### Test Accuracy\n",
    "![](2b_figures/test_accuracy.png)\n",
    "#### Validation Accuracy\n",
    "![](2b_figures/val_accuracy.png)\n",
    "\n",
    "The two plots look identical, but when I hovered over specific data points in TensorBoard, I saw there was a slight difference (about a tenth of a percent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2c:  Time for More Fun!!!\n",
    "\n",
    "## Playing with nonlinearities\n",
    "\n",
    "### Using Tanh as an activation function\n",
    "\n",
    "#### Note:  Swapping all ReLUs with tanhs!\n",
    "\n",
    "![](2c_figures/tanh/h_conv1.png)\n",
    "\n",
    "What's nice with tanh that you can see in this plot is you don't get dead neurons!  The minimum activation after the first convolutional layer is far from zero, unlike with ReLU where it is stuck at ~1e-16 from the beginning.  The nonzero activation for tanh implies that all neurons are alive.\n",
    "\n",
    "While the test and validation accuracies (>95%) were quite high after the first epoch, diverse, not-quite-convergent behavior in the weights and biases is observed with tanh.\n",
    "\n",
    "![](2c_figures/tanh/W_fc1.png)\n",
    "![](2c_figures/tanh/b_conv1.png)\n",
    "\n",
    "\n",
    "\n",
    "### Using Sigmoid as an Activation Function\n",
    "\n",
    "![](2c_figures/tanh/test_accuracy.png)\n",
    "Only 82.7 percent after the first epoch!\n",
    "\n",
    "However, weights and activations look pretty smooth.  Frequent non-monotonic behavior.\n",
    "![](2c_figures/sigmoid/h_conv1.png)\n",
    "![](2c_figures/sigmoid/W_fc1.png)\n",
    "\n",
    "\n",
    "### Using Leaky ReLU (L-ReLU) as an Activation Function\n",
    "\n",
    " $\\mathbf{\\alpha} $ **is set to -0.01**\n",
    "\n",
    "\n",
    "![](2c_figures/LReLu/test_accuracy.png)\n",
    "\n",
    "Very fast accuracy of 96.32 after the first epoch.\n",
    "\n",
    "![](2c_figures/LReLu/h_conv.png)\n",
    "Summary statistics are much noisier, which is probably a good thing for exploration of the landscape.  No dead neurons.\n",
    "\n",
    "\n",
    "![](2c_figures/LReLu/input1_hist.png)\n",
    "This histogram is sharply peaked with wide tails!  what does this mean?\n",
    "\n",
    "\n",
    "\n",
    "### Using ELU as an Activation Function\n",
    "\n",
    "Exponential Linear Units (ELUs) are known to fit very fast.\n",
    "\n",
    "Test accuracy looks the same as the Leaky ReLU.\n",
    "\n",
    "![](2c_figures/ELU/h_conv1.png)\n",
    "Activation summaries mean, std, max, min, all appear to be dropping... not sure why they would all do this unless the neurons are all becoming way less active.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Playing with gradient descent optimizers\n",
    "\n",
    "\n",
    "\n",
    "### adagrad\n",
    "\n",
    "![](2c_figures/adagrad_loss.png)\n",
    "\n",
    "### vanilla SGD\n",
    "\n",
    "![](2c_figures/vanilla_loss.png)\n",
    "\n",
    "\n",
    "### Momentum\n",
    "\n",
    "![](2c_figures/adagrad_loss.png)\n",
    "\n",
    "It looks like loss is made worse with all optimizers examined other than Adam.\n",
    "(Adam was using for ReLU settings)\n",
    "\n",
    "Momentum is better than vanilla SGD, which no surprise, appears to be the worst of the bunch.\n",
    "\n",
    "## Playing with Xavier Initialization\n",
    "\n",
    "\n",
    "### Using Xavier Initialization type:  Uniform distribution\n",
    "![](2c_figures/xav_uniform.png)\n",
    "\n",
    "This give an extremely high accuracy 99.04 for test, 99.06 for validation in a very short amount of time (fast convergence).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
