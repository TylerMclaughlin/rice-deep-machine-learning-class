{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Figure_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of tanh\n",
    "\n",
    "$$\\tanh x = { \\frac{\\sinh x}{\\cosh x} }\n",
    " = { \\frac {e^{x}-e^{-x}}{e^{x} +e^{-x}} } $$\n",
    "\n",
    "\n",
    "$$\\frac{d}{dx}{\\sinh x} = \\frac{1}{2}*(\\frac{d}{dx}e^{x}-\\frac{d}{dx}e^{-x}) = \\frac{1}{2}*(e^{x} + e^{-x}) = \\cosh x $$\n",
    "$$\\frac{d}{dx}{\\cosh x} = \\frac{1}{2}*(\\frac{d}{dx}e^{x}+\\frac{d}{dx}e^{-x}) = \\frac{1}{2}*(e^{x} - e^{-x}) = \\sinh x. $$\n",
    "<p style=\"text-align: center;\">  \n",
    "Applying the quotient rule:\n",
    "</p>\n",
    "\n",
    "$$ \\frac{d}{dx}{\\tanh x} = \\frac{\\cosh x \\frac{d}{dx} \\sinh x - \\sinh x \\frac{d}{dx} \\cosh x } {{\\cosh}^{2} x} $$\n",
    "\n",
    "$$ = \\frac{\\cosh x \\cosh x - \\sinh x \\sinh x}{\\cosh^{2} x}$$\n",
    "\n",
    "$$ = 1 - \\frac{\\sinh ^{2}x}{\\cosh^{2}x} = 1 - \\tanh^{2} x $$\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of logistic sigmoid\n",
    "\n",
    "$$ \\sigma (x) =  \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "<p style=\"text-align: center;\">  \n",
    "Applying the chain rule:\n",
    "</p>\n",
    "$$ \\frac{d}{dx} \\sigma (x) = - \\frac{1}{(1 + e^{-x})^{2}} * -e^{-x} $$\n",
    "\n",
    "$$ = \\sigma (x) \\frac{e^{-x}}{1 + e^{-x}} $$\n",
    "\n",
    "$$ = \\sigma (x) (1 - \\sigma (x)) $$\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ReLU\n",
    "\n",
    "The Rectified Linear Unit is defined as $ f(x)=\\text{max}(0,x) $.  \n",
    "We can rewrite this using two cases:\n",
    "\n",
    "\\begin{equation} \n",
    "f(x)=\n",
    "    \\begin{cases}\n",
    "      x, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation} .\n",
    "\n",
    "Upon simple differentiation of the two cases, we get\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation} \n",
    "\n",
    "The ReLU is discontinuous at x = 0, \n",
    "therefore its derivative at x = 0 is technically not defined;\n",
    "however, we are explicitly setting $ f'(0) = 0 $ in the statement above, so we have defined a derivative $\\forall x \\in \\mathbb{R}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backward Pass - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Derivative 1:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial W^{(2)}}$.\n",
    "\n",
    "By the chain rule:\n",
    "\n",
    "$ = \\frac{\\partial L}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\frac{\\partial z^{(2)}}{\\partial W^{(2)}} $\n",
    "\n",
    "However, there is a mathematical trick where we can speed up backpropagation computations if we **compose** the softmax and cross-entropy.  This means we'll be looking at factors #1 and #2 together, $\\frac{\\partial L}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = \\frac{\\partial L}{\\partial z^{(2)}}$.  We will later add factor #3 $\\frac{\\partial z^{(2)}}{\\partial W^{(2)}}$ \n",
    "\n",
    "This trick was taken from the \"Deep Learning\" textbook by Goodfellow, Bengio, and Courville, page 199.\n",
    "\n",
    "### factors #1 and #2 together\n",
    "\n",
    "We can ignore N datapoints for now, and consider only the sum over classes $k \\in C$.\n",
    "\n",
    "Let's compute the following:\n",
    "$\\frac{\\partial L}{\\partial a^{(2)}} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(2)}} $\n",
    "\n",
    "Expressing the output layer's activations element-wise:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_{i}} = - \\sum\\limits_{k \\in C} y_{k} \\frac{\\partial log a_{k}}{\\partial z_{i}} = - \\sum\\limits_{k \\in C} y_{k} \\frac{1}{a_{k}} \\frac{\\partial  a_{k}}{\\partial z_{i}}$$\n",
    "\n",
    "\n",
    "Let's split the derivative into two cases:  \n",
    "\n",
    "#### case 1:  $ i = j$.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{i}}  = \\frac{\\partial}{\\partial z_{i}} \\frac{exp(z_{i})}{\\sum\\limits_{k \\in C}exp(z_{k})}$$\n",
    "\n",
    "By the quotient rule, we get:\n",
    "\n",
    "$$ \\frac{\\big[\\sum\\limits_{k}exp(z_{k})\\big]exp(z_{i}) - exp(z_{i})exp(z_{i}) }{ \\big[{\\sum\\limits_{k}exp(z_{k})}\\big]^{2} } $$.\n",
    "\n",
    "$$ =  \\frac{\\big[\\sum\\limits_{k}exp(z_{k})\\big] - exp(z_{i}) }{\\sum\\limits_{k}exp(z_{k})}  \\frac{exp(z_{i})}{\\sum\\limits_{k}exp(z_{k})} $$.\n",
    "\n",
    "This allows us to express the derivative of the softmax in terms of softmax function itself.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{i}} = [1 - softmax(z_{i})] softmax(z_{i})  = [1 - a_{i}]a_{i} $$\n",
    "\n",
    "Reincorporating into the full derivative above, we get:\n",
    "\n",
    "$$ - \\sum\\limits_{k = i} y_{k} \\frac{1}{a_{i}} \\frac{\\partial  a_{i}}{\\partial z_{i}}$$\n",
    "\n",
    "$$ =  -  y_{i} \\frac{1}{a_{i}} \\frac{\\partial  a_{i}}{\\partial z_{i}}$$\n",
    "\n",
    "$$ =  -  y_{i} \\frac{1}{a_{i}} [1 - a_{i}]a_{i}$$\n",
    "\n",
    "$$ = - y_{i} (1 - a_{i}) $$\n",
    "\n",
    "for i = j.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### case 2: $ i \\neq j$.\n",
    "\n",
    "$$ \\frac{\\partial  a_{i}}{\\partial z_{j}}  = \\frac{\\partial}{\\partial z_{j}} \\frac{exp(z_{i})}{\\sum\\limits_{k \\in C}exp(z_{k})}$$\n",
    "\n",
    "Using the quotient rule again:\n",
    "\n",
    "$$ \\frac{0 \\cdot exp(z_{i}) - exp(z_{i})exp(z_{j}) }{ \\big[{\\sum\\limits_{k}exp(z_{k})}\\big]^{2} }  = -\\frac{exp(z_{i})}{\\sum\\limits_{k}exp(z_{k})} \\frac{exp(z_{j})}{\\sum\\limits_{k}exp(z_{k})} = - softmax(z_{i})softmax(z_{j}) = -a_{i}a_{j}$$.\n",
    "\n",
    "Incorporating this into the derivative above,\n",
    "\n",
    "$$ - \\sum\\limits_{i \\neq j \\in C} y_{j} \\frac{1}{a_{j}} \\frac{\\partial  a_{j}}{\\partial z_{i}} =  -\\sum\\limits_{i \\neq j \\in C} y_{j} \\frac{1}{a_{j}} (-a_{j} a_{i}) =  \\sum\\limits_{i \\neq j \\in C} y_{j} a_{i} $$.\n",
    "\n",
    "#### combining two cases:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z_{i}} = - y_{i} (1 - a_{i}) + \\sum\\limits_{j \\neq i \\in C} y_{j} a_{i} $$\n",
    "$$ =  - y_{i} +  y_{i}a_{i} + \\sum\\limits_{j \\neq i \\in C} y_{j} a_{i} $$\n",
    "\n",
    "$$ = -y_{i} + \\sum\\limits_{j \\in C} y_{j}a_{i} $$\n",
    "\n",
    "Rearranging and then finally realizing that since y is a one-hot vector, it sums to 1, we get \n",
    "$$ = a_{i}\\sum\\limits_{j \\in C}y_{j} - y_{i}  $$\n",
    "\n",
    "$$ = a_{i} \\cdot 1 - y_{i}   = a_{i}^{(2)} - y_{i}$$.\n",
    "\n",
    "So the **gradient** of the loss function with respect to a particular z_{i} is quite simple mathematically and conceptually.  It is the difference between the true class identity pre-specified in y and the current activation a_{i} found by training at the ith ouput neuron. \n",
    "\n",
    "### factor #3\n",
    "\n",
    "We have $\\frac{\\partial L}{\\partial z_{i}^{(2)}}$ and we want the matrix $ \\frac{\\partial L}{\\partial W^{(2)}} $.  The chain rule tells us that $ \\frac{\\partial L}{\\partial W_{w}^{(2)}} = \\sum\\limits_{i}\\frac{\\partial L}{\\partial z_{i}^{(2)}}\\frac{\\partial z_{i}^{(2)}}{\\partial W_{w}^{(2)}},$ where $w$ represents a tuple of two indices (flattened matrix/tensor notation). \n",
    "\n",
    "Thus, to finish this part, we need $\\frac{\\partial z_{i}^{(2)}}{\\partial W_{w}}$.\n",
    "\n",
    "The connection between the hidden layer to the input of the output layer is given in terms of the activation at each of the hidden neurons, plus the weights, and biases:\n",
    "\n",
    "$ \\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.$\n",
    "\n",
    "$ \\mathbf{a}^{(2)} $  is an $N \\times H$ matrix and $\\mathbf{W}^{(2)}$  is an $H \\times C$ matrix, where N is the number of data points, H is the number of hidden layers, and C is the number of categories (output neurons).\n",
    "\n",
    "$ \\mathbf{z}^{(2)} = \\begin{bmatrix} a_{0,0} & a_{0,1} & \\dots & a_{0,H} \\\\ \\vdots  & \\ddots & & \\vdots \\\\ a_{N,0} & \\dots & & a_{N,H} \\end{bmatrix} \\begin{bmatrix} W_{0,0} & W_{0,1} & \\dots & W_{0,C} \\\\ W_{1,0} & W_{1,1} & & W_{1,C} \\\\  \\vdots & & \\ddots & \\vdots \\\\ W_{H,0}  & \\dots  & & W_{H,C}   \\end{bmatrix}  + \\mathbf{b}^{(2)} $ \n",
    "\n",
    "$ = \\begin{bmatrix} a_{0,0}W_{0,0} + a_{0,1}W_{1,0} + a_{0,2}W_{2,0} + ... + a_{0,H}W_{H,0} & \\dots & a_{0,0}W_{0,C} + a_{0,1}W_{1,C} + a_{0,2}W_{2,C} + ... + a_{0,H}W_{H,C} \\\\ \\vdots & \\ddots & \\vdots \\\\ \n",
    "a_{N,0}W_{0,0} + a_{N,1}W_{1,0} + a_{N,2}W_{2,0} + ... + a_{N,H}W_{H,0} & \\dots & a_{N,0}W_{0,C} + a_{N,1}W_{1,C} + a_{N,2}W_{2,C} + ... + a_{N,H}W_{H,C}\\end{bmatrix} + \\mathbf{b}^{(2)} $\n",
    "\n",
    "We only care about $z_{i}^{(2)}$, not the full matrix $\\mathbf{z}^{(2)}$ so we can consider the ith column of this matrix:\n",
    "\n",
    "$\\begin{bmatrix} a_{0,0}W_{0,i} + a_{0,1}W_{1,i} + a_{0,2}W_{2,i} + ... + a_{0,H}W_{H,i}  \\\\ \\vdots \\\\ a_{N,0}W_{0,i} + a_{N,1}W_{1,i} + a_{N,2}W_{2,i} + ... + a_{N,H}W_{H,i}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "We can calculate $\\frac{\\partial z_{i}^{(2)}}{\\partial W_{w}}$ by considering two cases, depending on the tuple of indices $w$.\n",
    "\n",
    "#### case 1:  $w = ( h, j), h \\in H, j = i $\n",
    "\n",
    "Taking the derivative causes every term  to vanish in each element of the column vector $z_{i}^{(2)}$ *except for a single term*.  This term is equal to a_{n,h}.\n",
    "\n",
    "\n",
    "\n",
    "#### case 2:  $w = ( h, j), h \\in H, j \\neq i$\n",
    "\n",
    "In this case, the derivative is simply equal to zero.\n",
    "\n",
    "#### full matrix of partial derivatives of $z_{j}$ with respect to the weights\n",
    "\\begin{equation} \n",
    "\\frac{\\partial z_{i}^{(2)}}{\\partial W_{h,j}}=\n",
    "    \\begin{cases}\n",
    "      a_{h}^{(1)}, & \\text{if}\\ j = i \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation} .\n",
    "\n",
    "\n",
    "### Finishing the chain rule for $\\frac {\\partial L }{\\partial \\mathbf{W}^{(2)}} $\n",
    "\n",
    "Recall, we are trying to find the derivative of the loss function with respect to each of the weights.\n",
    "\n",
    "$$\\frac{\\partial L}{W_{i,j}} = \\sum\\limits_{k}\\frac{\\partial L}{\\partial z^{(2)}_{k}}\\frac{\\partial z^{(2)}_{k}}{\\partial W_{i,j}} $$\n",
    "\n",
    "We may write this compactly as $ \\nabla_\\mathbf{W} L.$\n",
    "\n",
    "Let's run through a few example entries in this matrix to get a feel for the pattern.\n",
    "\n",
    "#### example entry:  $W_{0,0}$\n",
    "\n",
    "$$\\frac{\\partial L}{W_{0,0}} = \\sum\\limits_{k}\\frac{\\partial L}{\\partial z^{(2)}_{k}}\\frac{\\partial z^{(2)}_{k}}{\\partial W_{0,0}}$$ \n",
    "\n",
    "Let's expand the sum over k.  Our multi-layer neural network has only two possible values for k (two classes), so the sum is simple:\n",
    "\n",
    "$$ = \\frac{\\partial L}{\\partial z^{(2)}_{0}}\\frac{\\partial z^{(2)}_{0}}{\\partial W_{0,0}} + \\frac{\\partial L}{\\partial z^{(2)}_{1}}\\frac{\\partial z^{(2)}_{1}}{\\partial W_{0,0}}$$ \n",
    "\n",
    "Let's next substitute what we derived above analytically for the first partial derivative:\n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0})\\frac{\\partial z^{(2)}_{0}}{\\partial W_{0,0}}  + (a_{1}^{(2)} - y_{1})\\frac{\\partial z^{(2)}_{1}}{\\partial W_{0,0}}$$  \n",
    "\n",
    "and then substitute what we derived for the second partial derivative:\n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0}) \\cdot a_{0}^{(1)}  + (a_{1}^{(2)} - y_{1}) \\cdot 0$$  \n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0}) \\cdot a_{0}^{(1)}  .$$\n",
    "\n",
    "OK great.  Let's take a look at another element of $ \\nabla_\\mathbf{W} L.$\n",
    "\n",
    "#### example entry:  $W_{0,1}$\n",
    "\n",
    "$$ \\frac{\\partial L}{W_{0,1}} = \\frac{\\partial L}{\\partial z^{(2)}_{0}}\\frac{\\partial z^{(2)}_{0}}{\\partial W_{0,1}} + \\frac{\\partial L}{\\partial z^{(2)}_{1}}\\frac{\\partial z^{(2)}_{1}}{\\partial W_{0,1}}$$ \n",
    "\n",
    "Substitution is straightforward.  Observe how the cancellation is different:\n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0})\\cdot 0  + (a_{1}^{(2)} - y_{1}) \\cdot a_{0}^{(1)} $$  \n",
    "\n",
    "$$ = (a_{1}^{(2)} - y_{1})\\cdot a_{0}^{(1)} $$  \n",
    "\n",
    "We can do one more example before generalizing and constructing the full matrix:\n",
    "\n",
    "#### example entry:  $W_{1,0}$\n",
    "\n",
    "$$ \\frac{\\partial L}{W_{1,0}} = \\frac{\\partial L}{\\partial z^{(2)}_{0}}\\frac{\\partial z^{(2)}_{0}}{\\partial W_{1,0}} + \\frac{\\partial L}{\\partial z^{(2)}_{1}}\\frac{\\partial z^{(2)}_{1}}{\\partial W_{1,0}}$$ \n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0})\\frac{\\partial z^{(2)}_{0}}{\\partial W_{1,0}}  + (a_{1}^{(2)} - y_{1})\\frac{\\partial z^{(2)}_{1}}{\\partial W_{1,0}}$$  \n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0})\\cdot a_{1}  + (a_{1}^{(2)} - y_{1})\\cdot 0 $$  \n",
    "\n",
    "$$ = (a_{0}^{(2)} - y_{0}) \\cdot a_{1}^{(1)} $$\n",
    "\n",
    "#### generalizing:\n",
    "\n",
    "$$ (\\nabla_\\mathbf{W} L)_{i,j}  =  \\frac{\\partial L}{\\partial W_{i,j}^{(2)} } = (a_{j}^{(2)} - y_{j}) \\cdot a_{i}^{(1)}$$\n",
    "\n",
    "for $ i \\in C,$ the number of output layers, and $j \\in H ,$ the number of hidden layers.\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative 2:\n",
    "\n",
    "Calculate $ \\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}},$  where $\\mathbf{b}^{(2)}$ is a $ 1 \\times C$ bias matrix.\n",
    "\n",
    "### chain rule\n",
    "\n",
    "$$\\frac{\\partial L}{b_{i}^{(2)}} = \\sum\\limits_{k \\in C}\\frac{\\partial L}{\\partial z^{(2)}_{k}}\\frac{\\partial z^{(2)}_{k}}{\\partial b_{i}^{(2)}} = (a_{0}^{(2)} - y_{0})\\frac{\\partial z^{(2)}_{0}}{\\partial b_{i}^{(2)}}  + (a_{1}^{(2)} - y_{1})\\frac{\\partial z^{(2)}_{1}}{\\partial b_{i}^{(2)}}$$\n",
    "\n",
    "Recall that $\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\cdot \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} $.\n",
    "\n",
    "If we let $ \\mathbf{q} = \\mathbf{a}^{(1)} \\cdot \\mathbf{W}^{(2)} $, then  $\\mathbf{z}^{(2)} = \\begin{bmatrix} q_{0,0} & \\dots & q_{0,C} \\\\\n",
    "\\vdots  & \\ddots &  \\vdots \\\\\n",
    "q_{N,0} & \\dots & q_{N,C}\n",
    "\\end{bmatrix} + \\begin{bmatrix} b_{0}^{(2)} & \\dots & b_{C}^{(2)}\\end{bmatrix} $ \n",
    "\n",
    "Note:  technically $\\mathbf{b}^{(2)}$ should be an $ N \\times C$ dimensional matrix to get the summed terms to agree, but NumPy doesn't care about this and repeats (a.k.a. recycles) along the $N$ rows, yielding:\n",
    "\n",
    "$\\mathbf{z}^{(2)} = \\begin{bmatrix} q_{0,0} + b_{0}^{(2)} & \\dots & q_{0,C} + b_{C}^{(2)} \\\\\n",
    "\\vdots  & \\ddots &  \\vdots \\\\\n",
    "q_{N,0} + b_{0}^{(2)} & \\dots & q_{N,C} + b_{C}^{(2)}\n",
    "\\end{bmatrix} .$  \n",
    "\n",
    "So each $ z_{i}^{(2)} $ is a column of this matrix.   We note that $z_{i}^{(2)}$ does not depend on $b_{j}$ for $i \\neq j$ and thus $ \\frac{\\partial z^{(2)}_{i}}{\\partial b_{j}^{(2)}} = 0.$  Otherwise, if $ i = j$, then $\\frac{\\partial z^{(2)}_{i}}{\\partial b_{i}^{(2)}} = 1.$ \n",
    "\n",
    "### substituting into the chain rule:\n",
    "\n",
    "Just as we did before for $\\frac{\\partial L}{\\partial W_{i,j}}$, let's look at a few specific cases before generalizing.\n",
    "\n",
    "$$\\frac{\\partial L}{b_{i}^{(2)}} = \\sum\\limits_{k \\in C}\\frac{\\partial L}{\\partial z^{(2)}_{k}}\\frac{\\partial z^{(2)}_{k}}{\\partial b_{i}^{(2)}} = (a_{0}^{(2)} - y_{0})\\frac{\\partial z^{(2)}_{0}}{\\partial b_{i}^{(2)}}  + (a_{1}^{(2)} - y_{1})\\frac{\\partial z^{(2)}_{1}}{\\partial b_{i}^{(2)}} + \\dots + (a_{C}^{(2)} - y_{C})\\frac{\\partial z^{(2)}_{1}}{\\partial b_{i}^{(2)}}$$.  Every term vanishes except where $ i = j$, in which case, that term is equal to $(a_{i}^{(2)} - y_{i})$.  \n",
    "\n",
    "Thus, $\\frac{\\partial L}{b_{i}^{(2)}} = (a_{i}^{(2)} - y_{i})$.\n",
    "\n",
    "$$ \\blacksquare . $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative 3:\n",
    "\n",
    "Here we calculate $ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}}$, where $\\mathbf{W}^{(1)}$ is an $I \\times H$ matrix with $H$ representing the number of hidden nodes and $I$ representing the number of input nodes.  This derivative is also a matrix.\n",
    "\n",
    "Let $a$ be the element-wise activation function.  Let $a'$ be its derivative.\n",
    "\n",
    "First, we can work on the gradient of $ z_{i}^{(2)} $ (the input to arbitrary node in layer 2) with respect to  $ z_{j}^{(1)}$ (the input to arbitrary node in layer 1).\n",
    "\n",
    "$$ \\frac{\\partial z_{i}^{(2)}}{\\partial z_{j}^{(1)}} = \\sum\\limits_{p}\\frac{\\partial z_{i}^{(2)}}{\\partial a_{p} } \\frac{\\partial a_{p}}{\\partial z_{j}^{(1)}}  $$\n",
    "\n",
    "where $p$ is a tuple of indices and $a$ is the $N x H$ matrix of activations.\n",
    "\n",
    "Recall the equation for the input to the second layer: \n",
    "$$\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} $$\n",
    "\n",
    "If we take a  single index of z, we get the following:\n",
    "\n",
    "$$ z_{i}^{(2)} = z  $$\n",
    "\n",
    "## deprecated\n",
    "\n",
    "We write the chain rule for each element in $\\mathbf{W}^{(1)}$ :\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial W_{w}^{(1)}} =  \\frac{\\partial L}{\\partial W^{(2)}_{i,j}} \\frac{\\partial W^{(2)}_{i,j}}{\\partial A} \\frac{\\partial A}{\\partial W_{w}^{(1)}} $ , where $w$ is a tuple of indices.\n",
    "\n",
    "First let's work on $\\frac{\\partial L}{\\partial A}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative 4:\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial b_{1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our Three Layer Neural Network\n",
    "\n",
    "## Experimenting with Activation Functions\n",
    "\n",
    "### Hyperbolic Tangent Activation Function\n",
    "![](Figure_1.E.1.tanh.png)\n",
    "\n",
    "### Sigmoid Activation Function\n",
    "\n",
    "![](1e_figures/Figure_1.E.1.sigmoid.png)\n",
    "\n",
    "This, to my eyes, looks **identical** to using the tangent activation!\n",
    "\n",
    " Note:  I tried re-implementing the sigmoid function calculation with a stable algorithm (Scipy built-in) to avoid overflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Poincare/kinsen/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "z = -1e3\n",
    "# OLD CALCULATION\n",
    "activation = 1. / (1 + np.exp(-z))  # yields overflow for z = -1e3\n",
    "activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEW CALCULATION using Scipy.special.expit\n",
    "activation = scipy.special.expit(z)  # does not overflow\n",
    "activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Function\n",
    "\n",
    "![](1e_figures/Figure_1.E.1.relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Varying the number of hidden nodes\n",
    "\n",
    "### 1 Node\n",
    "![](1e_figures/Figure_1.E.1.tanh.1-nodes.png)\n",
    "\n",
    "1 node does as well as a simple multiple regression.\n",
    "### 2 Nodes\n",
    "![](1e_figures/Figure_1.E.1.tanh.2-nodes.png)\n",
    "\n",
    "2 nodes are able to fit the data slightly better than a single linear decision split.\n",
    "### 4 Nodes\n",
    "![](1e_figures/Figure_1.E.1.tanh.4-nodes.png)\n",
    "\n",
    "At 4 nodes, we see a proper fit to the crescent shape of the data.\n",
    "\n",
    "### 6 Nodes\n",
    "![](1e_figures/Figure_1.E.1.tanh.6-nodes.png)\n",
    "\n",
    "6 nodes gives a lower loss than 4 nodes, classifying the points more accurately, although at this point we appear to be overfitting to the random jitter.\n",
    "\n",
    "### 9 Nodes\n",
    "![](1e_figures/Figure_1.E.1.tanh.9-nodes.png)\n",
    "\n",
    "Adding more than 6 hidden nodes does not appear to improve the quality of the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Training a Simple Deep Convolutional Network on MNIST\n",
    "\n",
    "## Part a) Build and Train a 4-layer DCN\n",
    "\n",
    "### conv1(5-5-1-32) - ReLU - maxpool(2-2) - conv2(5-5-32-64) - ReLU - maxpool(2-2) fc(1024) - ReLU - DropOut(0.5) - Softmax(10)\n",
    "\n",
    "All my code for this section is in the file dcn_mnist_part2a.py\n",
    "\n",
    "Last few lines of the terminal output:\n",
    "\n",
    "step 4900, training accuracy 0.98\n",
    "step 5000, training accuracy 0.96\n",
    "step 5100, training accuracy 1\n",
    "step 5200, training accuracy 1\n",
    "step 5300, training accuracy 0.96\n",
    "step 5400, training accuracy 1\n",
    "test accuracy 0.9869\n",
    "The training takes 1014.218075 second to finish\n",
    "\n",
    "About 17 minutes to run!  Not bad on my macbook pro.\n",
    "\n",
    "After running the file I moved my results directory to results_part_2a/\n",
    "\n",
    "#### visualized results\n",
    "This shows the training loss value as a function of iteration number.\n",
    "![](2a_figures/scalars.png)\n",
    "![](2a_figures/graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b) More on Visualizing Your Training\n",
    "\n",
    "### Let's show some select figures.  All the figures are on my Github page github.com/TylerMcLaughlin\n",
    "\n",
    "![](2b_figures/W_conv1.png)\n",
    "![](2b_figures/b_conv1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
